{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"k7xBVPzoXxOg"},"source":["# Unit 3: Deep Q-Learning with Atari Games üëæ using RL Baselines3 Zoo"]},{"cell_type":"markdown","metadata":{"id":"ykJiGevCMVc5"},"source":["### üéÆ Environments: \n","\n","- SpacesInvadersNoFrameskip-v4 \n","\n","### üìö RL-Library: \n","\n","- [RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jV6wjQ7Be7p5"},"outputs":[],"source":["%%capture\n","!apt install python-opengl\n","!apt install ffmpeg\n","!apt install xvfb\n","!pip3 install pyvirtualdisplay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWyKJCy_NJBX"},"outputs":[],"source":["# Additional dependencies for RL Baselines3 Zoo\n","!apt-get install swig cmake freeglut3-dev "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5LwHrISW7Q5"},"outputs":[],"source":["!pip install pyglet==1.5.1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ww5PQH1gNLI4"},"outputs":[],"source":["# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"]},{"cell_type":"markdown","metadata":{"id":"mYIMvl5X9NAu"},"source":["## Clone RL-Baselines3 Zoo Repo üìö\n","You can now directly install from python package `pip install rl_zoo3` but since we want **the full installation with extra environments and dependencies** we're going to clone `RL-Baselines3-Zoo` repository and install from source."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eu5ZDPZ09VNQ"},"outputs":[],"source":["!git clone https://github.com/DLR-RM/rl-baselines3-zoo"]},{"cell_type":"markdown","metadata":{"id":"HCIoSbvbfAQh"},"source":["## Install dependencies üîΩ\n","We can now install the dependencies RL-Baselines3 Zoo needs (this can take 5min ‚è≤)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2QsFAk29h-D"},"outputs":[],"source":["%cd /content/rl-baselines3-zoo/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3QaOS7Xj9j1s"},"outputs":[],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"5iPgzluo9z-u"},"source":["## Train our Deep Q-Learning Agent to Play Space Invaders üëæ\n","\n","To train an agent with RL-Baselines3-Zoo, we just need to do two things:\n","1. We define the hyperparameters in `rl-baselines3-zoo/hyperparams/dqn.yml`\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/hyperparameters.png\" alt=\"DQN Hyperparameters\">\n"]},{"cell_type":"markdown","metadata":{"id":"5qTkbWrkECOJ"},"source":["In terms of hyperparameters optimization, my advice is to focus on these 3 hyperparameters:\n","- `learning_rate`\n","- `buffer_size (Experience Memory size)`\n","- `batch_size`\n","\n","As a good practice, you need to **check the documentation to understand what each hyperparameters does**: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Hn8bRTHvERRL"},"source":["2. We run `train.py` and save the models on `logs` folder üìÅ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xr1TVW4xfbz3"},"outputs":[],"source":["!python train.py --algo dqn  --env SpaceInvadersNoFrameskip-v4 -f logs/"]},{"cell_type":"markdown","metadata":{"id":"_dLomIiMKQaf"},"source":["## Let's evaluate our agent üëÄ\n","- RL-Baselines3-Zoo provides `enjoy.py`, a python script to evaluate our agent. In most RL libraries, we call the evaluation script `enjoy.py`.\n","- Let's evaluate it for 5000 timesteps üî•"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"co5um_KeKbBJ"},"outputs":[],"source":["!python enjoy.py  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/"]},{"cell_type":"markdown","metadata":{"id":"liBeTltiHJtr"},"source":["## Publish our trained model on the Hub üöÄ\n","Now that we saw we got good results after the training, we can publish our trained model on the hub ü§ó with one line of code.\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/space-invaders-model.gif\" alt=\"Space Invaders model\">"]},{"cell_type":"markdown","metadata":{"id":"XMSeZRBiHk6X"},"source":["To be able to share your model with the community there are three more steps to follow:\n","\n","1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n","\n","2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n","- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">"]},{"cell_type":"markdown","metadata":{"id":"9O6FI0F8HnzE"},"source":["- Copy the token \n","- Run the cell below and past the token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ppu9yePwHrZX"},"outputs":[],"source":["from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n","notebook_login()\n","!git config --global credential.helper store"]},{"cell_type":"markdown","metadata":{"id":"2RVEdunPHs8B"},"source":["If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`"]},{"cell_type":"markdown","metadata":{"id":"dSLwdmvhHvjw"},"source":["3Ô∏è‚É£ We're now ready to push our trained agent to the ü§ó Hub üî•"]},{"cell_type":"markdown","metadata":{"id":"PW436XnhHw1H"},"source":["Let's run push_to_hub.py file to upload our trained agent to the Hub.\n","\n","`--repo-name `: The name of the repo\n","\n","`-orga`: Your Hugging Face username\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ygk2sEktTDEw"},"outputs":[],"source":["!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4 -orga aj555 -f logs/"]},{"cell_type":"markdown","metadata":{"id":"ff89kd2HL1_s"},"source":["Congrats ü•≥ you've just trained and uploaded your first Deep Q-Learning agent using RL-Baselines-3 Zoo. The script above should have displayed a link to a model repository such as https://huggingface.co/ThomasSimonini/dqn-SpaceInvadersNoFrameskip-v4. When you go to this link, you can:\n","\n","- See a **video preview of your agent** at the right. \n","- Click \"Files and versions\" to see all the files in the repository.\n","- Click \"Use in stable-baselines3\" to get a code snippet that shows how to load the model.\n","- A model card (`README.md` file) which gives a description of the model and the hyperparameters you used.\n","\n","Under the hood, the Hub uses git-based repositories (don't worry if you don't know what git is), which means you can update the model with new versions as you experiment and improve your agent.\n","\n","**Compare the results of your agents with your classmates** using the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) üèÜ"]},{"cell_type":"markdown","metadata":{"id":"fyRKcCYY-dIo"},"source":["## Load a powerful trained model üî•\n","- The Stable-Baselines3 team uploaded **more than 150 trained Deep Reinforcement Learning agents on the Hub**.\n","\n","You can find them here: üëâ https://huggingface.co/sb3\n","\n","Some examples:\n","- Asteroids: https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4\n","- Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n","- Breakout: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4\n","- Road Runner: https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4\n","\n","Let's load an agent playing Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-9QVFIROI5Y"},"outputs":[],"source":["%%html\n","<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"]},{"cell_type":"markdown","metadata":{"id":"7ZQNY_r6NJtC"},"source":["1. We download the model using `rl_zoo3.load_from_hub`, and place it in a new folder that we can call `rl_trained`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdBNZHy0NGTR"},"outputs":[],"source":["# Download model and save it into the logs/ folder\n","!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/"]},{"cell_type":"markdown","metadata":{"id":"LFt6hmWsNdBo"},"source":["2. Let's evaluate if for 5000 timesteps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOxs0rNuN0uS"},"outputs":[],"source":["!python enjoy.py --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/"]},{"cell_type":"markdown","metadata":{"id":"kxMDuDfPON57"},"source":["Why not trying to train your own **Deep Q-Learning Agent playing BeamRiderNoFrameskip-v4? üèÜ.**\n","\n","If you want to try, check https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4#hyperparameters **in the model card, you have the hyperparameters of the trained agent.**"]},{"cell_type":"markdown","metadata":{"id":"xL_ZtUgpOuY6"},"source":["But finding hyperparameters can be a daunting task. Fortunately, we'll see in the next Unit, how we can **use Optuna for optimizing the Hyperparameters üî•.**\n"]},{"cell_type":"markdown","metadata":{"id":"-pqaco8W-huW"},"source":["## Some additional challenges üèÜ\n","The best way to learn **is to try things by your own**!\n","\n","In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?\n","\n","Here's a list of environments you can try to train your agent with:\n","- BeamRiderNoFrameskip-v4\n","- BreakoutNoFrameskip-v4 \n","- EnduroNoFrameskip-v4\n","- PongNoFrameskip-v4\n","\n","Also, **if you want to learn to implement Deep Q-Learning by yourself**, you definitely should look at CleanRL implementation: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>"]},{"cell_type":"markdown","metadata":{"id":"paS-XKo4-kmu"},"source":["________________________________________________________________________\n","Congrats on finishing this chapter!\n","\n","If you‚Äôre still feel confused with all these elements...it's totally normal! **This was the same for me and for all people who studied RL.**\n","\n","Take time to really **grasp the material before continuing and try the additional challenges**. It‚Äôs important to master these elements and having a solid foundations.\n","\n","In the next unit, **we‚Äôre going to learn about [Optuna](https://optuna.org/)**. One of the most critical task in Deep Reinforcement Learning is to find a good set of training hyperparameters. And Optuna is a library that helps you to automate the search.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5WRx7tO7-mvC"},"source":["\n","\n","### This is a course built with you üë∑üèø‚Äç‚ôÄÔ∏è\n","\n","Finally, we want to improve and update the course iteratively with your feedback. If you have some, please fill this form üëâ https://forms.gle/3HgA7bEHwAmmLfwh9\n","\n","We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues)."]},{"cell_type":"markdown","metadata":{"id":"Kc3udPT-RcXc"},"source":["See you on Bonus unit 2! üî• "]},{"cell_type":"markdown","metadata":{"id":"fS3Xerx0fIMV"},"source":["### Keep Learning, Stay Awesome ü§ó"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe0KwPtx5iGP"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[{"file_id":"https://github.com/huggingface/deep-rl-class/blob/master/notebooks/unit3/unit3.ipynb","timestamp":1673867327647}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6 (v3.10.6:9c7b4bd164, Aug  1 2022, 17:13:48) [Clang 13.0.0 (clang-1300.0.29.30)]"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
